{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logisitc算法的newtonRapshon推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文档基于我整理的 [logisitc算法的gradient ascent推导](http://nbviewer.jupyter.org/github/ChenShicong/logisitc/blob/master/gradient_logisitc_step.ipynb)，因此继续沿用梯度上升中的符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预备信息  \n",
    "$X: 自变量矩阵, n \\times p$, 即$X$的维度为n行p列。    \n",
    "$\\beta : 系数向量, p \\times 1$,即$\\beta$维度为p行1列。对应于代码里边的weights    \n",
    "$y: 因变量向量, n \\times 1$,即$y$的维度为n行1列。    \n",
    "$p: 预测概率向量, n \\times 1$,即$p$的维度为n行1列。\n",
    "即:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    \\mathbf{X} = \\left(\n",
    "      \\begin{array}{ccc}\n",
    "        x_{11} & x_{12} & \\ldots & x_{1p}\\\\\n",
    "        x_{21} & x_{22} & \\ldots  & x_{2p}\\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        x_{n1} & x_{n2} & \\ldots & x_{np}\n",
    "      \\end{array} \\right) ,\\quad\n",
    "       \\mathbf{y} = \\left(\n",
    "      \\begin{array}{ccc}\n",
    "        y_{1} \\\\\n",
    "        y_{2} \\\\\n",
    "        \\vdots \\\\\n",
    "        y_{n} \n",
    "      \\end{array} \\right) ,\\quad\n",
    "       \\mathbf{\\beta} = \\left(\n",
    "      \\begin{array}{ccc}\n",
    "        \\beta_{1} \\\\\n",
    "        \\beta_{2} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta_{p} \n",
    "      \\end{array} \\right), \\quad\n",
    "       \\mathbf{p} = \\left(\n",
    "      \\begin{array}{ccc}\n",
    "        p_{1} \\\\\n",
    "        p_{2} \\\\\n",
    "        \\vdots \\\\\n",
    "        p_{n} \n",
    "      \\end{array} \\right)\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\triangledown J(\\beta)$ : $J(\\beta)$关于向量$\\beta$的一阶导.    \n",
    "$H J(\\beta)$: $J(\\beta)$关于向量$\\beta$的二阶导.  即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    \\mathbf{H J(\\beta)} = \\left(\n",
    "      \\begin{array}{ccc}\n",
    "        \\frac{\\partial J^2(\\beta)}{\\partial \\beta_1^2} & \\frac{\\partial J^2(\\beta)}{\\partial \\beta_1 \\beta_2} & \\ldots &\\frac{\\partial J^2(\\beta)}{\\partial \\beta_1 \\beta_p}\\\\\n",
    "        \\frac{\\partial J^2(\\beta)}{\\partial \\beta_2 \\beta_1} & \\frac{\\partial J^2(\\beta)}{\\partial \\beta_2^2} & \\ldots  &  \\frac{\\partial J^2(\\beta)}{\\partial \\beta_2 \\beta_p}\\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        \\frac{\\partial J^2(\\beta)}{\\partial \\beta_n \\beta_1} & \\frac{\\partial J^2(\\beta)}{\\partial \\beta_n \\beta_2} & \\ldots & \\frac{\\partial J^2(\\beta)}{\\partial \\beta_p^2}\n",
    "      \\end{array} \\right) ,\\quad\n",
    "       \\mathbf{\\triangledown J(\\beta)} = \\left(\n",
    "      \\begin{array}{ccc}\n",
    "         \\frac{\\partial J(\\beta)}{\\partial \\beta_1} \\\\\n",
    "        \\frac{\\partial J(\\beta)}{\\partial \\beta_2} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\partial J(\\beta)}{\\partial \\beta_p}\n",
    "      \\end{array} \\right)\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在梯度上升求解的文档中，我们给出了logisitc对数似然函数:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\beta) = \\sum_{i=1}^n {y_i}log(p_{i})  + {(1-y_i)}log(1-p_i), \\quad p_i = \\frac{1}{1+exp{(-X^{(i)} \\beta)}}, \\quad X^{(i)}为矩阵X的第i行$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以及logisitc对数似然函数的一阶导数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\beta)}{\\partial \\beta_j} =  \\sum_{i=1}^n (y_i - p_i) \\cdot x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下我们给出logisitc对数似然函数的二阶导数:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial J^2(\\beta)}{\\partial \\beta_j \\partial \\beta_k} &= \\frac{\\partial \\frac{\\partial J(\\beta)}{\\partial \\beta_j}}{\\partial \\beta_k} \\\\\n",
    "& = \\frac{\\partial \\left[\\sum_{i=1}^n (y_i - p_i) \\cdot x_{ij} \\right]}{\\partial \\beta_k}\\\\ \n",
    "& = \\frac{\\partial \\left[\\sum_{i=1}^n ( - p_i \\cdot x_{ij}) \\right]}{\\partial \\beta_k}\\\\\n",
    "& =  \\frac{\\partial \\left[\\sum_{i=1}^n ( - \\frac{1}{1+exp{(-X^{(i)} \\beta)}} \\cdot x_{ij}) \\right]}{\\partial \\beta_k}\\\\\n",
    "& = -\\sum_{i=1}^n -\\frac{x_{ij}}{(1+exp{(-X^{(i)} \\beta)})^2} \\cdot exp{(-X^{(i)} \\beta)} \\cdot -x_{ik}\\\\\n",
    "& = -\\sum_{i=1}^n x_{ij} x_{ik} \\cdot \\frac{exp{(-X^{(i)} \\beta)}}{1+exp{(-X^{(i)} \\beta)}}  \\cdot \\frac{1}{1+exp{(-X^{(i)} \\beta)}}\\\\\n",
    "& = \\sum_{i=1}^n x_{ij} x_{ik} \\cdot -\\frac{exp{(-X^{(i)} \\beta)}}{1+exp{(-X^{(i)} \\beta)}}  \\cdot \\frac{1}{1+exp{(-X^{(i)} \\beta)}}\\\\\n",
    "& = \\sum_{i=1}^n x_{ij} x_{ik} (p_{i} - 1)p_{i}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton–Raphson method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单一变量的一阶导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们考虑单一的一个变量的泰勒一阶展开，这里我们考虑$f{(x)}$于$x = x_o$处的一阶展开:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = f(x_0) + f'(x_0) (x - x_0)$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个式子其实也就是$f$于$x = x_0$时的切线方程，我们令$f(x) = 0$, 即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$0 = f(x_0) + f'(x_0) (x - x_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整理可得此切线与x轴的横坐标："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = x_0 - \\frac{f(x_0)}{f'(x_0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为方便推算，我们记这一次得到的横坐标为$x_1$，即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，我们重新计算$f(x)$在$x_1$处的切线方程，可得："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算$f(x)$在$x_2$处的切线方程，可得："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_3 = x_2 - \\frac{f(x_2)}{f'(x_2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$......$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递推可得："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{n+1} = x_{n} - \\frac{f(x_{n})}{f'(x_{n})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从而，当n足够大时，我们必能得到一个$x^*$满足$f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单一变量的二阶导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们考虑单一的一个变量的泰勒二阶展开，这里我们考虑$f{(x)}$于$x = x_o$处的二阶展开:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = f(x_0) + f'(x_0) (x - x_0) + \\frac{f^{''}(x_0)}{2} (x-x_0)^2 + o(x-x_0)^2$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "省去高阶无穷小项，令$x$无穷趋近于$x_0$时，我们可以得到:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x_0) (x - x_0) + \\frac{f^{''}(x_0)}{2} (x-x_0)^2 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$x$无穷趋近于$x_0$时，可以注意到上式中的$\\frac{1}{2}$对于一个趋于无穷小的项几乎没有影响，所以我们可以将上式改写为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x_0) (x - x_0) + f^{''}(x_0) (x-x_0)^2 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整理上式，可得:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = x_0 - \\frac{f^{'}(x_0)}{f^{''}(x_0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，我们可以记这个新的$x$为$x_1$，重复**单一变量的一阶导**的步骤，类似地，我们可以得到以下迭代更新式子:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{n+1} = x_{n} - \\frac{f^{'}(x_{n})}{f{''}(x_{n})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多维向量的二阶导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个多维向量$X$, 以及在点$X_0$的邻域内有连续二阶偏导数的多元函数$f(X)$, 可以写出该函数在点$X_0$处的二阶泰勒展开式:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(X) = f(X_0) + (X - X_0)^T \\cdot \\triangledown f(X_0) + \\frac{1}{2} (X-X_0)^T \\cdot Hf(X_0) \\cdot (X-X_0) + o(||X-X_0||^2))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\triangledown f(X_0)$指的是$f$于$X=X_0$时的一阶导，$Hf(X_0)$指的是$f$于$X=X_0$时的二阶导，即是一个Hessian矩阵。 $o(||X-X_0||^2)$是高阶无穷小表示的皮亚诺余项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：** 上边式子中的$X$以及$X_0$均为多维向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比一下**单一变量的二阶导**，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = f(x_0) + f'(x_0) (x - x_0) + \\frac{f^{''}(x_0)}{2} (x-x_0)^2 + o(x-x_0)^2$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现这是极其相似的，同样的处理方法，当X无穷逼近于$X_0$时，忽略掉无穷小项，即得到迭代公式:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X_{n+1} = X_{n} - \\frac{\\triangledown f(X_n)}{Hf(X_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此，将$X_{n+1}$和$X_{n}$改写为logisitic对数似然函数里的$\\beta$，将$\\triangledown f(X_n)$改写为$\\triangledown J(\\beta)$, 将$Hf(X_n)$ 改写为$H J(\\beta)$我们可以得到logisitc系数的**newtonRapshon**更新式子:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic的newtonRapshon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\beta = \\beta - \\frac{\\triangledown J(\\beta)}{H J(\\beta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从预测信息里边，我们证明了:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J(\\beta)}{\\partial \\beta_j} =  \\sum_{i=1}^n (y_i - p_i) \\cdot x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J^2(\\beta)}{\\partial \\beta_j \\partial \\beta_k}  = \\sum_{i=1}^n x_{ij} x_{ik} (p_{i} - 1)p_{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这两个式子矢量化，矩阵化，代入$\\beta = \\beta - \\frac{\\triangledown J(\\beta)}{H J(\\beta)}$中，得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\beta = \\beta - (X^T A X)^{-1} X^T (y - p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    \\mathbf{A} = \\left(\n",
    "      \\begin{array}{ccc}\n",
    "        p_1(1-p_1) & 0 & \\ldots & 0\\\\\n",
    "        0 & p_2(1-p_2) & \\ldots  &0\\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "        0 & 0 & \\ldots & p_n(p_n - 1)\n",
    "      \\end{array} \\right) \n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "证毕."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
